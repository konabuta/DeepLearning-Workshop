{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 設備の残存耐用時間(RUL)を予測する時系列モデリング\n",
    "本Notebookでは、豊富な計算環境が用意されているAzure Machine Learning service の Machine Learning Compute のコンピューティング環境を用いて、高速に深層学習(LSTM)を行います。設備の残存耐用時間を予測する時系列モデルを構築します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 故障予測のアプローチ方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "故障予測のアプローチ方法は色々ありますが、代表的なアプローチを下記に記載しました。本Notebookでは、設備の残存耐用時間(RUL)を予測する深層学習モデルを構築するアプローチを採用しています。いずれのアプローチにも言えることですが、故障を予測するのではなく、故障する予兆を予測することが大事です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../docs/images/RUL.png\" align=\"left\" width=550>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用するデータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../docs/images/PowerBI-RUL.png\" align=\"left\" width=550>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure ML Workspaceへ接続\n",
    "Azure Machine Learning service ワークスペースへ接続します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "\n",
    "subscription_id = '9c0f91b8-eb2f-484c-979c-15848c098a6b'\n",
    "resource_group = 'mlservice'\n",
    "workspace_name = 'azureml'\n",
    "\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験名の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(workspace = workspace, name = \"lstm-rul-aml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クラウドにデータをアップロード\n",
    "学習で使用するデータをオンプレミスからクラウドにアップロードします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 2 files\n",
      "Target already exists. Skipping upload for data/test.csv\n",
      "Target already exists. Skipping upload for data/train.csv\n",
      "Uploaded 0 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_170552d02f6e432b85c2287a0c508596"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = workspace.get_default_datastore()\n",
    "ds.upload(src_dir='./data', target_path='data', overwrite=False, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習コード準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "project_folder = \"./keras-lstm\"\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./keras-lstm/keras_lstm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {project_folder}/keras_lstm.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.python.keras.layers import Activation, Dropout, Flatten, Input, Dense, Dropout, LSTM\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from azureml.core import Run\n",
    "from azureml.core import Workspace, Dataset\n",
    "from keras.utils import plot_model\n",
    "import argparse\n",
    "\n",
    "\n",
    "#from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "\n",
    "\n",
    "np.random.seed(1234)  \n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "from azureml.core import Run\n",
    "run = Run.get_context()\n",
    "\n",
    "parser = argparse.ArgumentParser(description='keras lstm example:')\n",
    "parser.add_argument('--epochs', '-e', type=int, default=10, help='Number of sweeps over the dataset to train')\n",
    "parser.add_argument('--batchsize', '-b', type=int, default=32, help='Number of images in each mini-batch')\n",
    "parser.add_argument('--dataset', '-d', dest='data_folder',help='The datastore')\n",
    "args = parser.parse_args()\n",
    "\n",
    "train_df = pd.read_csv(args.data_folder+\"/data/train.csv\", sep=\",\", header=0)\n",
    "train_df['RUL'] = train_df['RUL'].astype(float)\n",
    "test_df = pd.read_csv(args.data_folder+\"/data/test.csv\", sep=\",\", header=0)\n",
    "train_df['RUL'] = train_df['RUL'].astype(float)\n",
    "\n",
    "sequence_length = 50\n",
    "\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    #指定された列の値を取得\n",
    "    data_array = id_df[seq_cols].values\n",
    "    #num_elements : 特定idのデータ数 (for id = 1, it is 192)\n",
    "    num_elements = data_array.shape[0]\n",
    "    # for id = 1, zip from both range(0, 142) & range(50, 192)\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        #print(start,stop)\n",
    "        yield data_array[start:stop, :]\n",
    "        \n",
    "        \n",
    "#  特徴量となる列の抽出 \n",
    "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "# 学習データのsequences作成\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) for id in train_df['id'].unique())\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "\n",
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    data_array = id_df[label].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    return data_array[seq_length:num_elements, :]\n",
    "\n",
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['label1']) \n",
    "             for id in train_df['id'].unique()]\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "epochs=args.epochs\n",
    "batch_size=args.batchsize\n",
    "validation_split=0.05\n",
    "\n",
    "\n",
    "# Hyper-Parameter\n",
    "run.log(\"エポック数\",epochs)\n",
    "run.log(\"バッチサイズ\",batch_size)\n",
    "run.log(\"検証データ分割\",validation_split)\n",
    "\n",
    "\n",
    "class RunCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, run):\n",
    "        self.run = run\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        print(\"test\")\n",
    "        self.run.log(name=\"training_loss\", value=float(logs.get('loss')))\n",
    "        self.run.log(name=\"validation_loss\", value=float(logs.get('val_loss')))\n",
    "        self.run.log(name=\"training_acc\", value=float(logs.get('acc')))\n",
    "        self.run.log(name=\"validation_acc\", value=float(logs.get('val_acc')))\n",
    "\n",
    "callbacks = list()\n",
    "callbacks.append(RunCallback(run))\n",
    "\n",
    "# モデルネットワークの定義\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "print(\"nb_features:\",seq_array.shape[2])\n",
    "print(\"nb_out:\",label_array.shape[1])\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=nb_out, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(x = seq_array, y = label_array, epochs=epochs, batch_size=batch_size, validation_split=validation_split, verbose=1,\n",
    "          callbacks = callbacks)\n",
    "\n",
    "\n",
    "\n",
    "# training metrics\n",
    "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
    "run.log(\"損失\",scores[0])\n",
    "run.log(\"モデル精度\", scores[1])\n",
    "\n",
    "os.makedirs('./outputs/model', exist_ok=True)\n",
    "model.save_weights('./outputs/mnist_mlp_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Compute設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning Computeの設定を行います。GPUの場合は**gpucluster**、CPUの場合は**cpucluster**を指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "compute_target = ComputeTarget(workspace,\"gpucluster\")\n",
    "#compute_target = ComputeTarget(ws,\"cpucluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル学習設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowのEstimatorの設定を行います。GPUでモデル学習する際は、use_gpu = Trueに設定します。 CPUしか利用できない場合は、このパラメーターを削除するか、user_gpu=False に設定しなおします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n",
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--dataset': ds.as_mount()\n",
    "}\n",
    "\n",
    "estimator = TensorFlow(source_directory=project_folder,\n",
    "                       compute_target=compute_target,\n",
    "                       entry_script='keras_lstm.py',\n",
    "                       script_params=script_params,\n",
    "                       framework_version = '1.13',\n",
    "                       pip_packages = ['keras'],\n",
    "                      )\n",
    "\n",
    "# estimator = Estimator(source_directory=project_folder,\n",
    "#                        compute_target=compute_target,\n",
    "#                        entry_script='keras_lstm.py',\n",
    "#                        script_params=script_params,\n",
    "#                        pip_packages = ['pandas','tensorflow==2.0.0','keras'],\n",
    "#                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実行開始"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記で定義した TensorFlow Estimator の設定に従って、トレーニング環境を構築し、モデル学習を始めます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(Experiment: lstm-rul-aml,\n",
      "Id: lstm-rul-aml_1570551179_4e858781,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Preparing)\n"
     ]
    }
   ],
   "source": [
    "run = experiment.submit(estimator)\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69dd283d1a6348f5bf9e504656e9bf8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル無事完了したことを確認して、次に進みます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル登録"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azureml-logs/20_image_build_log.txt',\n",
       " 'azureml-logs/55_azureml-execution-tvmps_f9943600cc420db1ea2d23af1032def313e315926abc3a41d9e0d58404fe485b_p.txt',\n",
       " 'azureml-logs/65_job_prep-tvmps_f9943600cc420db1ea2d23af1032def313e315926abc3a41d9e0d58404fe485b_p.txt',\n",
       " 'azureml-logs/70_driver_log.txt',\n",
       " 'azureml-logs/75_job_post-tvmps_f9943600cc420db1ea2d23af1032def313e315926abc3a41d9e0d58404fe485b_p.txt',\n",
       " 'logs/azureml/152_azureml.log',\n",
       " 'logs/azureml/azureml.log',\n",
       " 'outputs/mnist_mlp_weights.h5']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUL-lstm-keras\tRUL-lstm-keras:7\t7\n"
     ]
    }
   ],
   "source": [
    "model = run.register_model(model_name = 'RUL-lstm-keras', model_path = 'outputs/mnist_mlp_weights.h5',tags = {'area': \"turbine predictive maintenance\", 'type': \"lstm\"})\n",
    "print(model.name, model.id, model.version, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ハイパーパラメータチューニング  Hyperdrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning Compute を用いて複数サーバでパラメータチューニングを分散で実行します。今回は Random Search を用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.hyperdrive.policy import BanditPolicy\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice\n",
    "    \n",
    "\n",
    "param_sampling = RandomParameterSampling( {\n",
    "    \"--batchsize\": choice(32, 64, 128, 256),\n",
    "    \"--epochs\": choice(5, 10, 20, 40, 80)\n",
    "    }\n",
    ")\n",
    "\n",
    "hyperdrive_run_config = HyperDriveConfig(estimator=estimator,\n",
    "                                            hyperparameter_sampling=param_sampling, \n",
    "                                            primary_metric_name='validation_acc',\n",
    "                                            primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                            max_total_runs=4,\n",
    "                                            max_concurrent_runs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run = experiment.submit(hyperdrive_run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365543a6ee404174abdf7c034e7c3670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
